# Summary: Deep Learning for AI

Deep learning mimics the mechanics of human intelligence by using multiple layers of vectors, which proved a success in many fields of Machine Learning.

This passage summarizes the history, recent developments and future obstacles.

**From Hand-Coded Symbolic Expressions to Learned Distributed Representations**

AI has two paradigms: the logic-inspired and the brain-inspired, respectively focusing on symbolic deduction using hardwired rules and simbols and brain simulation that modifies its own weights in learning.

The internal representation of symbols is different in the two paradigms. Its representation the logic-inspired paradigm is meaningless unless a relational graph is given, while that of the brain-inspired one is a vector, giving rise to rich similarity structures. Researches have demonstrated the capability of the latter in learning setence structures.

The brain-inspired paradigm surpass the logic-inspired paradigm in automatic generalization, which aid analogical reasoning.

**The Rise of Deep Learning**

The contributors to the rise of deep learning are: GPU, large datasets, and handy software platforms. 

**Why depth?** Deep neural networks outperform shallow ones with similar number of parameters, as confirmed by practice.

Why? Because deep networks can compose data in a more complicated way to capture abstract features.

Biological perception systems rely on similar mechanisms.

**Unsupervised pre-training** refers to the use of label-less data to pre-train the neural network to extract features from input, utilized especially when labeled data are few. The case for transfer learning has subtle differences.

In practice, after several layers are pre-trained, they are fine-tuned using labeled data.

